DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
sparseTitle = removeSparseTerms(frequenciesTitle, 0.98)
sparseBody = removeSparseTerms(frequenciesBody, 0.95)
# Step 10: Create data frame from the document-term matrix, we have 45 variables for the title,
# and 88 variables for the body
dataTitle = as.data.frame(as.matrix(sparseTitle))
dataBody = as.data.frame(as.matrix(sparseBody))
colnames(dataTitle) = make.names(colnames(dataTitle))
colnames(dataBody) = make.names(colnames(dataBody))
# Step 11: Join the independent variables together
duplicatedBody = dataBody[, names(dataBody) %in% names(dataTitle)]
duplicatedTitle = dataTitle[, names(dataTitle) %in% names(dataBody)]
duplicatedTotal = duplicatedBody + duplicatedTitle
Bodynew = select(dataBody, -c(names(duplicatedTitle)))
Titlenew = select(dataTitle, -c(names(duplicatedTitle)))
datajoined = as.data.frame(cbind(duplicatedTotal,Bodynew,Titlenew))
datajoined$Useful = data$Useful
# Part b)
spl = sample.split(datajoined$Useful, SplitRatio = 0.7)
Train <- datajoined %>% filter(spl == TRUE)
Test <- datajoined %>% filter(spl == FALSE)
table(Train$Useful)
table(Test$Useful)
# Function to compute accuracy of a classification model
tableAccuracy <- function(test, pred) {
t = table(test, pred)
a = sum(diag(t))/sum(t)
return(a)
}
DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
train.cart$bestTune
mod.cart = train.cart$finalModel
prp(mod.cart)
predict.cart = predict(mod.cart, newdata = Test, type = "class")
table(Test$Useful, predict.cart)
tableAccuracy(Test$Useful, predict.cart)
mod.cart
# What about CART on training set?
PredictCARTTrain = predict(mod.cart, type = "class")
table(Train$Useful, PredictCARTTrain)
tableAccuracy(Train$Useful, PredictCARTTrain)
DataLog = glm(Useful ~ ., data = Train, family = "binomial")
summary(DataLog)
# Predictions on test set
PredictLog = predict(DataLog, newdata = Test, type = "response")
table(Test$Useful, PredictLog > 0.5)
tableAccuracy(Test$Useful, PredictLog > 0.5)
# But what about training set?
PredictLogTrain = predict(DataLog, type = "response")
table(Train$Useful, PredictLogTrain > 0.5)
tableAccuracy(Train$Useful, PredictLogTrain > 0.5)
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
tGrid = expand.grid(n.trees = (1:100)*50, interaction.depth = c(1,2,4,6,8,10,12,14,16),
shrinkage = 0.01, n.minobsinnode = 10)
train.boost <- train(Useful ~ .,
data = Train,
method = "gbm",
tuneGrid = tGrid,
trControl = trainControl(method="cv", number=5, verboseIter = TRUE),
metric = "Accuracy",
distribution = "bernoulli")
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
### Bootstrap ###
library(boot)
mean_squared_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MSE <- mean((responses - predictions)^2)
return(MSE)
}
mean_absolute_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MAE <- mean(abs(responses - predictions))
return(MAE)
}
OS_R_squared <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
baseline <- data$baseline[index]
SSE <- sum((responses - predictions)^2)
SST <- sum((responses - baseline)^2)
r2 <- 1 - SSE/SST
return(r2)
}
all_metrics <- function(data, index) {
mse <- mean_squared_error(data, index)
mae <- mean_absolute_error(data, index)
OSR2 <- OS_R_squared(data, index)
return(c(mse, mae, OSR2))
}
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Useful))
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Useful))
Train$Useful
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = Train$Useful)
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Score)
###### Random Forests + Plots ######
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Score)
###### Random Forests + Plots ######
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Score)
###### Random Forests + Plots ######
###### Random Forests + Plots ######
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Score))
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Score))
# Step 5: Remove all punctuation
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusBody = tm_map(corpusBody, removePunctuation)
# Step 6: Remove stop words
# Remove stopword,these are words common in the title and body
corpusTitle = tm_map(corpusTitle, removeWords, c("apple", "ggplot","ggplot2",stopwords("english")))
corpusBody = tm_map(corpusBody, removeWords, c("apple", "ggplot","ggplot2",stopwords("english")))
# Step 7: Stem our document
# We chop off the ends of words that aren't maybe
# as necessary as the rest, like 'ing' and 'ed'
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusBody = tm_map(corpusBody, stemDocument)
# Step 8: Create a word count matrix (rows are each post, columns are words)
# We've finished our basic cleaning, so now we want to calculate frequencies
# of words across each post
frequenciesTitle = DocumentTermMatrix(corpusTitle)
frequenciesBody = DocumentTermMatrix(corpusBody)
# Step 9: Account for sparsity
# We currently have way too many words, which will make it hard to train
# our models and may even lead to overfitting.
# Use findFreqTerms to get a feeling for which words appear the most
# Words that appear at least 50 times:
findFreqTerms(frequenciesTitle, lowfreq=50)
findFreqTerms(frequenciesBody, lowfreq=50)
# Words that appear at least 20 times:
findFreqTerms(frequenciesTitle, lowfreq=20)
findFreqTerms(frequenciesBody, lowfreq=20)
# Our solution to the possibility of overfitting is to only keep terms
# that appear in x% or more of the posts. For example:
# 5% of the posts or more for the title, and 9% or more for the body
# Here we will get 45 variables for the title, and 88 variables for the body
sparseTitle = removeSparseTerms(frequenciesTitle, 0.98)
sparseBody = removeSparseTerms(frequenciesBody, 0.91)
# Step 10: Create data frame from the document-term matrix, we have 45 variables for the title,
# and 88 variables for the body
dataTitle = as.data.frame(as.matrix(sparseTitle))
dataBody = as.data.frame(as.matrix(sparseBody))
# We have some variable names that start with a number,
# which can cause R some problems. Let's fix this before going
# any further
colnames(dataTitle) = make.names(colnames(dataTitle))
colnames(dataBody) = make.names(colnames(dataBody))
# Step 11: Join the independent variables together
duplicatedBody = dataBody[, names(dataBody) %in% names(dataTitle)]
duplicatedTitle = dataTitle[, names(dataTitle) %in% names(dataBody)]
duplicatedTotal = duplicatedBody + duplicatedTitle
Bodynew = select(dataBody, -c(names(duplicatedTitle)))
Titlenew = select(dataTitle, -c(names(duplicatedTitle)))
datajoined = as.data.frame(cbind(duplicatedTotal,Bodynew,Titlenew))
# Step 12: Create new column to say a score is positive if it is greater than or equal to 1
datajoined$Useful = data$Useful
# Part b)
spl = sample.split(datajoined$Useful, SplitRatio = 0.7)
Train <- datajoined %>% filter(spl == TRUE)
Test <- datajoined %>% filter(spl == FALSE)
table(Train$Useful)
table(Test$Useful)
# Function to compute accuracy of a classification model
tableAccuracy <- function(test, pred) {
t = table(test, pred)
a = sum(diag(t))/sum(t)
return(a)
}
# Basic Random Forests:
DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
# Cross-validated CART model
train.cart = train(Useful ~ .,
data = Train,
method = "rpart",
tuneGrid = data.frame(cp=seq(0, 0.4, 0.002)),
trControl = trainControl(method="cv", number=10))
train.cart
train.cart$results
train.cart$bestTune
ggplot(train.cart$results, aes(x = cp, y = Accuracy)) +
geom_point(size = 2) +
geom_line() +
ylab("CV Accuracy") +
theme_bw() +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
mod.cart = train.cart$finalModel
prp(mod.cart)
predict.cart = predict(mod.cart, newdata = Test, type = "class")
table(Test$Useful, predict.cart)
tableAccuracy(Test$Useful, predict.cart)
# What about CART on training set?
PredictCARTTrain = predict(mod.cart, type = "class")
table(Train$Useful, PredictCARTTrain)
tableAccuracy(Train$Useful, PredictCARTTrain)
# Logistic Regression
DataLog = glm(Useful ~ ., data = Train, family = "binomial")
summary(DataLog)
# Predictions on test set
PredictLog = predict(DataLog, newdata = Test, type = "response")
table(Test$Useful, PredictLog > 0.5)
tableAccuracy(Test$Useful, PredictLog > 0.5)
# But what about training set?
PredictLogTrain = predict(DataLog, type = "response")
table(Train$Useful, PredictLogTrain > 0.5)
tableAccuracy(Train$Useful, PredictLogTrain > 0.5)
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
### Bootstrap ###
library(boot)
mean_squared_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MSE <- mean((responses - predictions)^2)
return(MSE)
}
mean_absolute_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MAE <- mean(abs(responses - predictions))
return(MAE)
}
OS_R_squared <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
baseline <- data$baseline[index]
SSE <- sum((responses - predictions)^2)
SST <- sum((responses - baseline)^2)
r2 <- 1 - SSE/SST
return(r2)
}
all_metrics <- function(data, index) {
mse <- mean_squared_error(data, index)
mae <- mean_absolute_error(data, index)
OSR2 <- OS_R_squared(data, index)
return(c(mse, mae, OSR2))
}
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = mean(Train$Useful))
Train$Useful
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = sum(Train$Useful)/len(Train))
DataRF2 <- train(letter ~.,
data = Train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:16),
trControl = trainControl(method="cv", number=10, verboseIter = TRUE),
metric = "Accuracy")
DataRF2 <- train(Useful ~.,
data = Train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:16),
trControl = trainControl(method="cv", number=10, verboseIter = TRUE),
metric = "Accuracy")
PredictRF2 = predict(DataRF2, newdata = Test)
table(Test$Useful, PredictRF2)
tableAccuracy(Test$Useful, PredictRF2)
View(datajoined)
DataRF2 <- train(Useful ~.,
data = Train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:10),
trControl = trainControl(method="cv", number=5, verboseIter = TRUE),
metric = "Accuracy")
PredictRF2 = predict(DataRF2, newdata = Test)
table(Test$Useful, PredictRF2)
tableAccuracy(Test$Useful, PredictRF2)
DataRF2.best <- DataRF2$finalModel
PredictRF2 = predict(DataRF2.best, newdata = Test)
table(Test$Useful, PredictRF2)
tableAccuracy(Test$Useful, PredictRF2)
DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
# Basic Random Forests:
DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
### Bootstrap ###
library(boot)
mean_squared_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MSE <- mean((responses - predictions)^2)
return(MSE)
}
mean_absolute_error <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
MAE <- mean(abs(responses - predictions))
return(MAE)
}
OS_R_squared <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
baseline <- data$baseline[index]
SSE <- sum((responses - predictions)^2)
SST <- sum((responses - baseline)^2)
r2 <- 1 - SSE/SST
return(r2)
}
all_metrics <- function(data, index) {
mse <- mean_squared_error(data, index)
mae <- mean_absolute_error(data, index)
OSR2 <- OS_R_squared(data, index)
return(c(mse, mae, OSR2))
}
###### Random Forests + Plots ######
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF, baseline = Train$Useful)
# Basic Random Forests:
DataRF = randomForest(Useful ~ ., data=Train)
PredictRF = predict(DataRF, newdata = Test)
table(Test$Useful, PredictRF)
tableAccuracy(Test$Useful, PredictRF)
###### Random Forests + Plots ######
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF)
# sanity check
all_metrics(RF_test_set, 1:1818)
mean((PredictRF - Test$Useful)^2)
mean(abs(PredictRF - Test$Useful))
OSR2(PredictRF, Test$Useful, Train$Useful)
all_metrics(RF_test_set, 1:1818)
TPR <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
contingency <- table(responses, predictions)
TPR <- contingency[2, 2]/(contingency[2,2]+contingency[2,1])
return(TPR)
}
FPR <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
contingency <- table(responses, predictions)
FPR <- contingency[1, 2]/(contingency[1, 2] + contingency[1, 1])
return(FPR)
}
Accuracy <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
t = table(responses, predictions)
a = sum(diag(t))/length(responses)
return(a)
}
all_metrics <- function(data, index) {
accuracy <- accuracy.fun(data, index)
tpr <- TPR.fun(data, index)
fpr <- FPR.fun(data, index)
return(c(accuracy, tpr, fpr))
}
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF)
# do bootstrap
RF_boot <- boot(RF_test_set, all_metrics, R = 10000)
RF_boot
library(boot)
TPR <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
contingency <- table(responses, predictions)
TPR <- contingency[2, 2]/(contingency[2,2]+contingency[2,1])
return(TPR)
}
FPR <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
contingency <- table(responses, predictions)
FPR <- contingency[1, 2]/(contingency[1, 2] + contingency[1, 1])
return(FPR)
}
Accuracy <- function(data, index) {
responses <- data$response[index]
predictions <- data$prediction[index]
t = table(responses, predictions)
a = sum(diag(t))/length(responses)
return(a)
}
all_metrics <- function(data, index) {
accuracy <- Accuracy(data, index)
tpr <- TPR(data, index)
fpr <- FPR(data, index)
return(c(accuracy, tpr, fpr))
}
RF_test_set = data.frame(response = Test$Useful, prediction = PredictRF)
# do bootstrap
RF_boot <- boot(RF_test_set, all_metrics, R = 10000)
RF_boot
# make plots
rf_boot_plot_results = data.frame(osr2estimates = RF_boot$t[,3], delta = RF_boot$t[,3] - RF_boot$t0[3])
ggplot(rf_boot_plot_results) + geom_histogram(aes(x = osr2estimates), binwidth = 0.005, color = "blue") +
ylab("Count") + xlab("Bootstrap OSR2 Estimate") + theme_bw() +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18))
# get confidence intervals (not manually)
boot.ci(RF_boot, index = 1, type = "basic")
boot.ci(RF_boot, index = 2, type = "basic")
boot.ci(RF_boot, index = 3, type = "basic")
RF_test_set2 = data.frame(response = Test$Useful, prediction = PredictRF2)
# do bootstrap
RF_boot2 <- boot(RF_test_set, all_metrics, R = 10000)
RF_boot2
# get confidence intervals (not manually)
boot.ci(RF_boot, index = 1, type = "basic")
boot.ci(RF_boot, index = 2, type = "basic")
boot.ci(RF_boot, index = 3, type = "basic")
RF_test_set2 = data.frame(response = Test$Useful, prediction = PredictRF2)
# do bootstrap
RF_boot2 <- boot(RF_test_set, all_metrics, R = 10000)
RF_boot2
# get confidence intervals (not manually)
boot.ci(RF_boot2, index = 1, type = "basic")
boot.ci(RF_boot2, index = 2, type = "basic")
boot.ci(RF_boot2, index = 3, type = "basic")
###### CART ######
CART_test_set = data.frame(response = Test$Useful, prediction = predict.cart)
# do bootstrap
CART_boot <- boot(CART_test_set, all_metrics, R = 10000)
CART_boot
# get confidence intervals
boot.ci(CART_boot, index = 1, type = "basic")
boot.ci(CART_boot, index = 2, type = "basic")
boot.ci(CART_boot, index = 3, type = "basic")
###### CART ######
CART_test_set = data.frame(response = Test$Useful, prediction = predict.cart)
# do bootstrap
CART_boot <- boot(CART_test_set, all_metrics, R = 10000)
CART_boot
# get confidence intervals
boot.ci(CART_boot, index = 1, type = "basic")
boot.ci(CART_boot, index = 2, type = "basic")
boot.ci(CART_boot, index = 3, type = "basic")
###### Logistic Regression ######
Log_test_set = data.frame(response = Test$Useful, prediction = PredictLog)
# do bootstrap
Log_boot <- boot(Log_test_set, all_metrics, R = 10000)
Log_boot
# get confidence intervals
boot.ci(Log_boot, index = 1, type = "basic")
boot.ci(Log_boot, index = 2, type = "basic")
boot.ci(Log_boot, index = 3, type = "basic")
###### Linear Discriminant Analysis ######
LDA_test_set = data.frame(response = Test$Useful, prediction = lda.mod)
# do bootstrap
LDA_boot <- boot(LDA_test_set, all_metrics, R = 10000)
LDA_boot
# get confidence intervals
boot.ci(LDA_boot, index = 1, type = "basic")
boot.ci(LDA_boot, index = 2, type = "basic")
boot.ci(LDA_boot, index = 3, type = "basic")
###### Linear Discriminant Analysis ######
LDA_test_set = data.frame(response = Test$Useful, prediction = predict.lda)
# do bootstrap
LDA_boot <- boot(LDA_test_set, all_metrics, R = 10000)
LDA_boot
# get confidence intervals
boot.ci(LDA_boot, index = 1, type = "basic")
boot.ci(LDA_boot, index = 2, type = "basic")
boot.ci(LDA_boot, index = 3, type = "basic")
###### Linear Discriminant Analysis ######
LDA_test_set = data.frame(response = Test$Useful, prediction = predict.lda)
# do bootstrap
LDA_boot <- boot(LDA_test_set, all_metrics, R = 10000)
LDA_boot
# get confidence intervals
boot.ci(LDA_boot, index = 1, type = "basic")
boot.ci(LDA_boot, index = 2, type = "basic")
boot.ci(LDA_boot, index = 3, type = "basic")
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
# Linear Discriminant Analysis
library(MASS)
lda.mod = lda(Useful ~ ., data = Train)
predict.lda = predict(lda.mod, newdata = Test)$class
table(Test$Useful, predict.lda)
tableAccuracy(Test$Useful, predict.lda)
###### Linear Discriminant Analysis ######
LDA_test_set = data.frame(response = Test$Useful, prediction = predict.lda)
# do bootstrap
LDA_boot <- boot(LDA_test_set, all_metrics, R = 10000)
LDA_boot
# get confidence intervals
boot.ci(LDA_boot, index = 1, type = "basic")
boot.ci(LDA_boot, index = 2, type = "basic")
boot.ci(LDA_boot, index = 3, type = "basic")
