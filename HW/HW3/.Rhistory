# Because the beta coefficient for totChol is 0.003109, then the odds is e^0.003109 = 1.003
# iii)
# Threshold value of p is 0.16 such that it is optimal to perscribe the medication to a patient if
# their 10 year CHD exceeds p
# iv)
predTest = predict(mod, newdata=mydata.test, type="response")
summary(predTest)
table(mydata.test$TenYearCHD, predTest > 0.16)
# Model Accuracy: (648+108)/(648+282+59+108) = 0.69. Given this, our model is some what accurate because
# this value shows that our model is accurate 69% of the time. The accuracy was found to be the proportion
# of the TenYearCHD that was correctly classified vs the TenYearCHD that  was incorrectly classified
# True Positive Rate (TPR): 108/(59+108) = 0.6467. The TPR is our estimate of the conditional probability that
# that our classifier makes a correct predicition given Y = 1. Proportion of patients that have CHD that we correctly
# prescribed with the medication
# False Positive Rate (FPR): 282/(648+282) = 0.303. The FPR is the proportion of patients that do not have CHD
# that we incorrectly prescribed the medicine for
# v)
Cost_per_patient <- ((560000*(0.16/4))+(60000*(1-(0.16/4))))/(282+108)
# Cost per patient is around $205.128
# This assumption is not reasonable because this assumption believes that the treatment decision is independent of a patient's risk of developing CHD.
# However, there is the possiility that the treatment decision could impact a patients risk of developing CHD
Cost_per_patient_adj <- ((560000*(0.16/4))+(60000*(1-(0.16/4)))+(500000*0.16)+(0*(1-0.16)))/(648+282+59+108)
# Adjusted cost per patient is around $145.852
# vi)
# Baseline model: predict that no one defaults
# Accuracy of baseline on training:
# Accuracy of baseline on testing:
table(mydata.test$TenYearCHD)
# Accuracy of baseline on testing:
table(Letters.test$Letters$isB)
Letters$isB = as.factor(Letters$letter == "B")
Letters.isB = as.factor(Letters$letter == "B")
Letters$isB = as.factor(Letters$letter == "B")
Letters$isB = as.factor(Letters$letter == "B")
Letters.isB = as.factor(Letters$letter == "B")
# IND242HW3 Problem 2
# Nicolas Kardous
library(dplyr)
library(ggplot2)
library(caTools) # splits
library(rpart) # CART
library(rpart.plot) # CART plotting
# Part a)
Letters = read.csv("Letters.csv")
Letters$isB = as.factor(Letters$letter == "B")
set.seed(456)
train.ids = sample(nrow(Letters), 0.65*nrow(Letters))
Letters.train = Letters[train.ids,]
Letters.test = Letters[-train.ids,]
# i)
# Baseline model: predict that it is "not B"
# Accuracy of baseline on test set:
# Accuracy of baseline on testing:
table(Letters.test$isB)
mod <- glm(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, family = "binomial")
summary(mod)
mod <- glm(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, family = "binomial")
summary(mod)
predTest = predict(mod, newdata=Letters.test, type="response")
summary(predTest)
table(mydata.test$TenYearCHD, predTest > 0.5)
mod <- glm(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, family = "binomial")
summary(mod)
predTest = predict(mod, newdata=Letters.test, type="response")
summary(predTest)
table(Letters.test$isB, predTest > 0.5)
rocr.log.pred <- prediction(predTest, Letters.test$isB)
logPerformance <- performance(rocr.log.pred, "tpr", "fpr")
plot(logPerformance, colorize = TRUE)
abline(0, 1)
as.numeric(performance(rocr.log.pred, "auc")@y.values)
# IND242HW3 Problem 2
# Nicolas Kardous
library(dplyr)
library(ggplot2)
library(caTools) # splits
library(rpart) # CART
library(rpart.plot) # CART plotting
# Part a)
Letters = read.csv("Letters.csv")
Letters$isB = as.factor(Letters$letter == "B")
set.seed(456)
train.ids = sample(nrow(Letters), 0.65*nrow(Letters))
Letters.train = Letters[train.ids,]
Letters.test = Letters[-train.ids,]
# i)
# Baseline model: predict that it is "not B"
# Accuracy of baseline on test set:
# Accuracy of baseline on testing:
table(Letters.test$isB)
# Model Accuracy: Number correct / Number in total = 788 / (788 + 303) =  0.7223
# The accuracy is 72.23%
# ii)
modLOG <- glm(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, family = "binomial")
summary(modLOG)
predTest = predict(modLOG, newdata=Letters.test, type="response")
summary(predTest)
table(Letters.test$isB, predTest > 0.5)
# Using a threshold value of p = 0.5, we find that the accuracy is (760 + 273) / (760 + 28 + 30  + 273)
# Thus, the accuracy = 0.9468 = 94.68%
# iii)
rocr.log.pred <- prediction(predTest, Letters.test$isB)
logPerformance <- performance(rocr.log.pred, "tpr", "fpr")
as.numeric(performance(rocr.log.pred, "auc")@y.values)
modCART <- rpart(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, method="class",
minbucket=5, cp = 0.001)
modCART
prp(modCART)
library(caret) # cross validation
install.packages('caret')
install.packages('e1071')
install.packages("caret")
library(caret) # cross validation
library(dplyr)
library(ggplot2)
library(caTools) # splits
library(rpart) # CART
library(rpart.plot) # CART plotting
library(caret) # cross validation
cpVals = data.frame(cp = seq(0, .04, by=.002))
train.cart <- train(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
View(cpVals)
train.cart$results
View(cpVals)
cpVals = data.frame(cp = seq(0, .1, by=.002))
train.cart <- train(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart$results
train.cart$bestTune
library(randomForest)
install.packages()
mod.rf <- randomForest(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.test)
data = Letters.test)
predTestrf = predict(mod.rf, newdata=Letters.test, type="response")
summary(predTestrf)
table(Letters.test$isB, predTestrf)
mod.rf <- randomForest(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.test)
predTestrf = predict(mod.rf, newdata=Letters.test, type="response")
summary(predTestrf)
table(Letters.test$isB, predTestrf)
mod.rf <- randomForest(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train)
predTestrf = predict(mod.rf, newdata=Letters.test, type="response")
table(Letters.test$isB, predTestrf)
table(Letters.test$letters)
# Accuracy of baseline on testing:
table(Letters.test$letter)
table(Letters$letter)
table(Letters.test$isB)
table(Letters$letter)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
table(letter, predTestLDA)
library(caret) # cross validation
library(ROCR)
library(MASS)
library(ROCR)
library(MASS)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
table(letter, predTestLDA)
table(Letters$isB, predTestrf)
table(Letters.test$letter, predTestLDA)
table(Letters.test$isB, predTestrf)
table(Letters.test, predTestLDA)
table(Letters.test$letter, predTestLDA)
table(letter, predTestLDA)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
table(Letters.test$letter, predTestLDA)
predTestLDA_probs <- predTestLDA$posterior[,2]
table(Letters.test$letter, predTestLDA)
cpVals2 = data.frame(cp = seq(0, .1, by=.002))
train.cart2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals2,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart2$results
train.cart2$results
train.cart2$bestTune
mod.bag <- randomForest(x = train.ctr.mm, y = train.ctr$CTR, mtry = 4, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = test.ctr.mm)
mod.bag <- randomForest(x = Letters.train, y = Letters.train$letter, mtry = 4, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table(Letters.test$letter, predbag)
table(Letters.test$letter, pred.bag)
train.rf2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:16),
trControl = trainControl(method="cv", number=10, verboseIter = TRUE),
metric = "Accuracy")
train.rf2$results
train.cart2$bestTune
train.rf2$bestTune
train.rf2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:16),
trControl = trainControl(method="cv", number=10, verboseIter = TRUE),
metric = "Accuracy")
train.rf2$bestTune
predTestrf2 <- predict(train.rf2, newdata = Letters.test)
table(Letters.test$letter, predTestrf2)
mod.boost <- gbm(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
distribution = "multinomial",
n.trees = 3300,
shrinkage = 0.001,
interaction.depth = 10)
library(gbm)
mod.boost <- gbm(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
distribution = "multinomial",
n.trees = 3300,
shrinkage = 0.001,
interaction.depth = 10)
pred.boost <- predict(mod.boost, newdata = Letters.test, type ="response")
pred.boost <- predict(mod.boost, newdata = Letters.test, n.trees=3300, type ="response")
pred.boost.class = apply(pred.boost, 1, which.max)
pred.boost.class = factor(pred.boost.class, levels = c(1,2,3,4), labels = c("A", "B", "P", "R"))
summary(mod.boost) # tells us what is the most influential variables.
table(Letters.test$letter, pred.boost.class)
table(Letters$letter)
table(Letters.test$isB)
modLOG <- glm(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data = Letters.train, family = "binomial")
summary(modLOG)
predTest = predict(modLOG, newdata=Letters.test, type="response")
summary(predTest)
table(Letters.test$isB, predTest > 0.5)
rocr.log.pred <- prediction(predTest, Letters.test$isB)
logPerformance <- performance(rocr.log.pred, "tpr", "fpr")
plot(logPerformance, colorize = TRUE)
as.numeric(performance(rocr.log.pred, "auc")@y.values)
abline(0,1)
cpVals = data.frame(cp = seq(0, .1, by=.002))
train.cart <- train(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart$results
train.cart$bestTune
ggplot(train.cart$results, aes(x=cp, y=Accuracy)) + geom_point(size=3) + xlab('Complexity Parameter (cp)') + geom_line()
train.cart <- train(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart$results
Letters.test.mm <- as.data.frame(model.matrix(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.test))
predCart <- predict(train.cart, newdata=Letters.test)
cartTable <- table(Letters.test$isB, predCart)
accuracy.cart.a <- sum(diag(cartTable))/sum(cartTable)
# Extract the best model and make predictions
best.cart = train.cart$finalModel
prp(best.cart, digits=3)
mod.rf <- randomForest(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train)
predTestrf = predict(mod.rf, newdata=Letters.test, type="response")
table(Letters.test$isB, predTestrf)
# Accuracy of baseline on testing:
table(Letters$letter)
table(Letters.train$letter)
# Accuracy of baseline on testing on the letter A:
Letters$isA = as.factor(Letters$letter=="A")
table(Letters.test$isA)
table(Letters.train$letter)
table(Letters.test$isA)
table(Letters.train$letter)
table(Letters.train$letter)
table(Letters.test$isA)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
table(Letters.test$letter, predTestLDA)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
predTestLDAclass <- predTestLDA$class
table(Letters.test$letter, predTestLDAclass)
LDA.table <- table(Letters.test$letter, predTestLDAclass)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
predTestLDAclass <- predTestLDA$class
LDA.table <- table(Letters.test$letter, predTestLDAclass)
LDA.accuracy <- sum(diag(LDA.table))/sum(LDA.table)
LdaModel <- lda(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train)
predTestLDA <- predict(LdaModel, newdata=Letters.test)
predTestLDAclass <- predTestLDA$class
LDA.table <- table(Letters.test$letter, predTestLDAclass)
LDA.table
LDA.accuracy <- sum(diag(LDA.table))/sum(LDA.table)
LDA.accuracy
cpVals2 = data.frame(cp = seq(0, .1, by=.001))
train.cart2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals2,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart2$results
train.cart2$bestTune
# First standard CV with respect to Accuracy
# method = specify classification method, "rpart" for CART
# tuneGrid = gives the sequence of parameters to try,
#             in this case, we try cp = 0 through cp=0.1 in increments of .002
# trControl = here using 10-fold cross validation
# metric = "Accuracy" for classification accuracy, "RMSE" or "Rsquared" or for regression
cpVals2 = data.frame(cp = seq(0, .05, by=.0001))
train.cart2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rpart",
tuneGrid = cpVals2,
trControl = trainControl(method = "cv", number=10),
metric = "Accuracy")
train.cart2$results
train.cart2$bestTune
ggplot(train.cart2$results, aes(x=cp, y=Accuracy)) + geom_point(size=3) + xlab('Complexity Parameter (cp)') + geom_line()
Letters.test.mm.b <- as.data.frame(model.matrix(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.test))
predCart.b <- predict(train.cart2, newdata=Letters.test)
cartTable.b <- table(Letters.test$isB, predCart.b)
accuracy.cart.b <- sum(diag(cartTable.b))/sum(cartTable.b)
# Extract the best model and make predictions
best.cart2 = train.cart$finalModel
prp(best.cart2, digits=3)
cartTable.b
Letters.test.mm.b <- as.data.frame(model.matrix(isB ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.test))
predCart.b <- predict(train.cart2, newdata=Letters.test)
cartTable.b <- table(Letters.test$letter, predCart.b)
cartTable.b
accuracy.cart.b <- sum(diag(cartTable.b))/sum(cartTable.b)
best.cart2 = train.cart$finalModel
prp(best.cart2, digits=3)
accuracy.cart.b
best.cart2 = train.cart2$finalModel
prp(best.cart2, digits=3)
cartTable.b <- table(Letters.test$letter, predCart.b)
cartTable.b
accuracy.cart.b <- sum(diag(cartTable.b))/sum(cartTable.b)
accuracy.cart.b
mod.bag <- randomForest(x = Letters.train, y = Letters.train$letter, mtry = 4, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table(Letters.test$letter, pred.bag)
mod.bag <- randomForest(x = Letters.train, y = Letters.train$letter, mtry = 16, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table(Letters.test$letter, pred.bag)
ggplot(train.cart2$results, aes(x=cp, y=Accuracy)) + geom_point(size=3) + xlab('Complexity Parameter (cp)') + geom_line()
predCart.b <- predict(train.cart2, newdata=Letters.test)
cartTable.b <- table(Letters.test$letter, predCart.b)
cartTable.b
accuracy.cart.b <- sum(diag(cartTable.b))/sum(cartTable.b)
accuracy.cart.b
Letters.train.mm <- as.data.frame(model.matrix(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train))
mod.bag <- randomForest(x = Letters.train, y = Letters.train$letter, mtry = 16, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table(Letters.test$letter, pred.bag)
Letters.train.mm <- as.data.frame(model.matrix(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train))
mod.bag <- randomForest(x = Letters.train.mm, y = Letters.train$letter, mtry = 16, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.abg))/sum(table.bag)
Letters.train.mm <- as.data.frame(model.matrix(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train))
mod.bag <- randomForest(x = Letters.train.mm, y = Letters.train$letter, mtry = 16, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.bag))/sum(table.bag)
pred.bag <- predict(mod.bag, newdata = Letters.test.mm.b)
Letters.train.mm <- as.data.frame(model.matrix(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.train))
Letters.test.mm.b <- as.data.frame(model.matrix(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor, data=Letters.test))
mod.bag <- randomForest(x = Letters.train.mm, y = Letters.train$letter, mtry = 16, nodesize = 5, ntree = 500)
pred.bag <- predict(mod.bag, newdata = Letters.test.mm.b)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.bag))/sum(table.bag)
pred.bag <- predict(mod.bag, newdata = Letters.test.mm.b)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.bag))/sum(table.bag)
pred.bag <- predict(mod.bag, newdata = Letters.test.mm.b)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.bag))/sum(table.bag)
accuracy.bag
pred.bag <- predict(mod.bag, newdata = Letters.test.mm.b)
table.bag <- table(Letters.test$letter, pred.bag)
table.bag
accuracy.bag <- sum(diag(table.bag))/sum(table.bag)
accuracy.bag
train.rf2 <- train(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
method = "rf", # random forest
tuneGrid = data.frame(mtry=1:16),
trControl = trainControl(method="cv", number=10, verboseIter = TRUE),
metric = "Accuracy")
train.rf2$results
train.rf2$bestTune
best.rf <- train.rf$finalModel
pred.best.rf <- predict(best.rf, newdata = Letters.test.mm.b)
rf.table <- table(Letters.test$letter, pred.best.rf)
rf.table
accuracy.rf <- sum(diag(rf.table))/sum(rf.table)
accuracy.rf
best.rf <- train.rf2$finalModel
pred.best.rf <- predict(best.rf, newdata = Letters.test.mm.b)
rf.table <- table(Letters.test$letter, pred.best.rf)
rf.table
accuracy.rf <- sum(diag(rf.table))/sum(rf.table)
accuracy.rf
# Set the type to "response"
pred.boost <- predict(mod.boost, newdata = Letters.test, n.trees=3300, type ="response")
# Convert matrix to a vector of class predictions
pred.boost.class = apply(pred.boost, 1, which.max)
pred.boost.class = factor(pred.boost.class, levels = c(1,2,3,4), labels = c("A", "B", "P", "R"))
summary(mod.boost) # tells us what is the most influential variables.
table.gbm <- table(Letters.test$letter, pred.boost.class)
table.gbm
accuracy.gbm <- sum(diag(table.gbm))/sum(table.gbm)
accuracy.gbm
mod.boost <- gbm(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
distribution = "multinomial",
n.trees = 3300,
interaction.depth = 10)
pred.boost <- predict(mod.boost, newdata = Letters.test, n.trees=3300, type ="response")
# Convert matrix to a vector of class predictions
pred.boost.class = apply(pred.boost, 1, which.max)
pred.boost.class = factor(pred.boost.class, levels = c(1,2,3,4), labels = c("A", "B", "P", "R"))
summary(mod.boost) # tells us what is the most influential variables.
table.gbm <- table(Letters.test$letter, pred.boost.class)
table.gbm
accuracy.gbm <- sum(diag(table.gbm))/sum(table.gbm)
accuracy.gbm
pred.boost <- predict(mod.boost, newdata = Letters.test, n.trees=3300, type ="response")
# Convert matrix to a vector of class predictions
pred.boost.class = apply(pred.boost, 1, which.max)
pred.boost.class = factor(pred.boost.class, levels = c(1,2,3,4), labels = c("A", "B", "P", "R"))
summary(mod.boost) # tells us what is the most influential variables.
table.gbm <- table(Letters.test$letter, pred.boost.class)
table.gbm
accuracy.gbm <- sum(diag(table.gbm))/sum(table.gbm)
accuracy.gbm
set.seed(456)
mod.boost <- gbm(letter ~ xbox + ybox + width + height + onpix + xbar + ybar + x2bar + y2bar + xybar + x2ybar
+ xy2bar + xedge + xedgeycor + yedge + yedgexcor,
data = Letters.train,
distribution = "multinomial",
n.trees = 3300,
interaction.depth = 10)
pred.boost <- predict(mod.boost, newdata = Letters.test, n.trees=3300, type ="response")
# Convert matrix to a vector of class predictions
pred.boost.class = apply(pred.boost, 1, which.max)
pred.boost.class = factor(pred.boost.class, levels = c(1,2,3,4), labels = c("A", "B", "P", "R"))
summary(mod.boost) # tells us what is the most influential variables.
table.gbm <- table(Letters.test$letter, pred.boost.class)
table.gbm
accuracy.gbm <- sum(diag(table.gbm))/sum(table.gbm)
accuracy.gbm
